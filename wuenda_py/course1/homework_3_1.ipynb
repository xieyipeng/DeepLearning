{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3周测验 - 浅层神经网络\n",
    "以下哪一项是正确的？\n",
    "\n",
    "【★】X是一个矩阵，其中每个列都是一个训练示例。\n",
    "【★】a[2]4a4[2] 是第二层第四层神经元的激活的输出。\n",
    "【★】a[2](12)a[2](12)表示第二层和第十二层的激活向量。\n",
    "【★】a[2]a[2] 表示第二层的激活向量。\n",
    "注意：如果您不熟悉本课程所使用的符号，请 戳我。\n",
    "博主注：我只列出了正确的答案。\n",
    "\n",
    "tanh激活函数通常比隐藏层单元的sigmoid激活函数效果更好，因为其输出的平均值更接近于零，因此它将数据集中在下一层是更好的选择，请问正确吗？\n",
    "\n",
    "【★】True\n",
    "【 】 False\n",
    "请注意，你可以看一下这篇文章 和这篇文档.\n",
    "\n",
    "As seen in lecture the output of the tanh is between -1 and 1, it thus centers the data which makes the learning simpler for the next layer.\n",
    "\n",
    "正如视频中所看到的，tanh的输出在-1和1之间，因此它将数据集中在一起，使得下一层的学习变得更加简单。\n",
    "\n",
    "其中哪一个是第l层向前传播的正确向量化实现，其中1≤l≤L1≤l≤L\n",
    "Z[l]=W[l]A[l−1]+b[l]Z[l]=W[l]A[l−1]+b[l]\n",
    "A[l]=g[l](Z[l])A[l]=g[l](Z[l])\n",
    "博主注：我只列出了正确的答案。\n",
    "\n",
    "您正在构建一个识别黄瓜（y = 1）与西瓜（y = 0）的二元分类器。 你会推荐哪一种激活函数用于输出层？\n",
    "\n",
    "【 】 ReLU\n",
    "【 】 Leaky ReLU\n",
    "【★】sigmoid\n",
    "【 】 tanh\n",
    "注意：来自sigmoid函数的输出值可以很容易地理解为概率。\n",
    "\n",
    "Sigmoid outputs a value between 0 and 1 which makes it a very good choice for binary classification. You can classify as 0 if the output is less than 0.5 and classify as 1 if the output is more than 0.5. It can be done with tanh as well but it is less convenient as the output is between -1 and 1.\n",
    "\n",
    "\n",
    "Sigmoid输出的值介于0和1之间，这使其成为二元分类的一个非常好的选择。 如果输出小于0.5，则可以将其归类为0，如果输出大于0.5，则归类为1。 它也可以用tanh来完成，但是它不太方便，因为输出在-1和1之间。\n",
    "\n",
    "看一下下面的代码：\n",
    "\n",
    "A = np.random.randn(4,3)\n",
    "B = np.sum(A, axis = 1, keepdims = True)\n",
    "1\n",
    "2\n",
    "请问B.shape的值是多少?\n",
    "\n",
    "B.shape = (4, 1)\n",
    "\n",
    "we use (keepdims = True) to make sure that A.shape is (4,1) and not (4, ). It makes our code more rigorous.\n",
    "\n",
    "\n",
    "我们使用（keepdims = True）来确保A.shape是（4,1）而不是（4，），它使我们的代码更加严格。\n",
    "\n",
    "假设你已经建立了一个神经网络。 您决定将权重和偏差初始化为零。 以下哪项陈述是正确的？\n",
    "\n",
    "【★】第一个隐藏层中的每个神经元节点将执行相同的计算。 所以即使经过多次梯度下降迭代后，层中的每个神经元节点都会计算出与其他神经元节点相同的东西。\n",
    "【 】 第一个隐藏层中的每个神经元将在第一次迭代中执行相同的计算。 但经过一次梯度下降迭代后，他们将学会计算不同的东西，因为我们已经“破坏了对称性”。\n",
    "【 】第一个隐藏层中的每一个神经元都会计算出相同的东西，但是不同层的神经元会计算不同的东西，因此我们已经完成了“对称破坏”。\n",
    "【 】即使在第一次迭代中，第一个隐藏层的神经元也会执行不同的计算， 他们的参数将以自己的方式不断发展。\n",
    "Logistic回归的权重w应该随机初始化，而不是全零，因为如果初始化为全零，那么逻辑回归将无法学习到有用的决策边界，因为它将无法“破坏对称性”，是正确的吗？\n",
    "\n",
    "【 】True\n",
    "【★】False\n",
    "Logistic Regression doesn’t have a hidden layer. If you initialize the weights to zeros, the first example x fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input x (because there’s no hidden layer) which is not zero. So at the second iteration, the weights values follow x’s distribution and are different from each other if x is not a constant vector.\n",
    "\n",
    "\n",
    "Logistic回归没有隐藏层。 如果将权重初始化为零，则Logistic回归中的第一个示例x将输出零，但Logistic回归的导数取决于不是零的输入x（因为没有隐藏层）。 因此，在第二次迭代中，如果x不是常量向量，则权值遵循x的分布并且彼此不同。\n",
    "\n",
    "您已经为所有隐藏单元使用tanh激活建立了一个网络。 使用np.random.randn（..，..）* 1000将权重初始化为相对较大的值。 会发生什么？\n",
    "\n",
    "【 】这没关系。只要随机初始化权重，梯度下降不受权重大小的影响。\n",
    "\n",
    "【 】这将导致tanh的输入也非常大，因此导致梯度也变大。因此，您必须将α设置得非常小以防止发散; 这会减慢学习速度。\n",
    "\n",
    "【 】这会导致tanh的输入也非常大，导致单位被“高度激活”，从而加快了学习速度，而权重必须从小数值开始。\n",
    "\n",
    "【★】这将导致tanh的输入也很大，因此导致梯度接近于零， 优化算法将因此变得缓慢。\n",
    "\n",
    "tanh becomes flat for large values, this leads its gradient to be close to zero. This slows down the optimization algorithm.\n",
    "\n",
    "\n",
    "tanh对于较大的值变得平坦，这导致其梯度接近于零。 这减慢了优化算法。\n",
    "\n",
    "看一下下面的单隐层神经网络：\n",
    "\n",
    "\n",
    "【★】b[1]b[1] 的维度是(4, 1)\n",
    "【★】W[1]W[1] 的维度是 (4, 2)\n",
    "【★】W[2]W[2] 的维度是 (1, 4)\n",
    "【★】b[2]b[2] 的维度是 (1, 1)\n",
    "博主注：我只列出了正确的答案。\n",
    "\n",
    "请注意: 点击这里 来看一下公式。\n",
    "\n",
    "I在和上一个相同的网络中，z[1]z[1] 和 A[1]A[1]的维度是多少？\n",
    "\n",
    "【★】z[1]z[1] 和 A[1]A[1] 的维度都是 (4,m)\n",
    "博主注：我只列出了正确的答案。\n",
    "\n",
    "请注意: 点击这里 来看一下公式。\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「何宽」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/u013733326/article/details/79866913"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
