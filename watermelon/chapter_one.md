* **基本术语**

  1. 数据集

  > 假定收集了一批关于西瓜的数据，（色泽=；根蒂=；敲声=；），（色泽=；根蒂=；敲声=；）。。。等等，这组记录的集合称为数据集。

  2. 属性

  > 反应事件或对象在在某方面的表现或性质的事项成为属性。

  3. 属性空间

  > 属性张成的空间称为属性空间。

  4. 特征向量

  > 我们把“色泽”，“根蒂”，“敲声”作为三个坐标轴，他们张成一个用于描述西瓜的三维空间。每个西瓜都可以在这个空间中找到自己的位置。由于空间中每个点对应一个坐标向量，因此吧一个示例称作一个“特征向量”。

  5. 样例

  > 如“（（色泽=；根蒂=；敲声=；），好瓜）”。这里关于结果的信息“好瓜“，称为标记，拥有标记信息的示例，称为样例。（xi，yi）表示第i个样例，yi属于Y，是样例xi的标记。Y是所有标记的集合，成为”标记空间“

  6. 离散值和连续值

  > 若预测的是离散值，此类学习任务称为”分类“，若是连续值，称为“回归”。“聚类”，即将训练集中的西瓜分成若干组，每组称为“簇”，分组一句有很多，如“浅色瓜”、”深色瓜“等。根据训练是否有标记信息，学习任务大致分为两类，“监督学习”和“无监督学习”，分类和回归是前者的代表，聚类是后者的代表。

  7. 泛化能力

  > 学得模型适用于新样本的能力称为泛化能力

* **假设空间与版本空间**

  * 定义

    ![img](https://img-blog.csdn.net/20170218222420369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTg0MzM0NDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

    ​

    > 版本空间(version space)是概念学习中与已知数据集一致的所有假设(hypothesis)的子集集合。

    > 对于二维空间中的“矩形”假设（上图），绿色加号代表正类样本，红色小圈代表负类样本。 GB 是最大泛化正假设边界(maximally General positive hypothesis Boundary), SB 是最大精确正假设边界(maximally Specific positive hypothesis Boundary). GB与SB所围成的区域中的矩形即为版本空间中的假设，也即GB与SB围成的区域就是版本空间。
    >
    > 在一些需要对假设的泛化能力排序的情形下，就可以通过GB与SB这两个上下界来表示版本空间。在学习的过程中，学习算法就可以只在GB、SB这两个代表集合上操作。

    ​

  * 示例

  ![主要符号表](https://s1.ax2x.com/2018/04/01/tf2B3.png)

  **表1.1的训练数据集对应的假设空间应该如下：**

  > 1 色泽＝＊，根蒂＝＊，敲声＝＊
  >
  > 2 色泽＝青绿，根蒂＝＊，敲声＝＊
  >
  > 3 色泽＝乌黑，根蒂＝＊，敲声＝＊
  >
  > 4 色泽＝＊，根蒂＝蜷缩，敲声＝＊
  >
  > 5 色泽＝＊，根蒂＝硬挺，敲声＝＊
  >
  > 6 色泽＝＊，根蒂＝稍蜷，敲声＝＊
  >
  > 7 色泽＝＊，根蒂＝＊，敲声＝浊响
  >
  > 8 色泽＝＊，根蒂＝＊，敲声＝清脆
  >
  > 9 色泽＝＊，根蒂＝＊，敲声＝沉闷
  >
  > 10 色泽＝青绿，根蒂＝蜷缩，敲声＝＊
  >
  > 11 色泽＝青绿，根蒂＝硬挺，敲声＝＊
  >
  > 12 色泽＝青绿，根蒂＝稍蜷，敲声＝＊
  >
  > 13 色泽＝乌黑，根蒂＝蜷缩，敲声＝＊
  >
  > 14 色泽＝乌黑，根蒂＝硬挺，敲声＝＊
  >
  > 15 色泽＝乌黑，根蒂＝稍蜷，敲声＝＊
  >
  > 16 色泽＝青绿，根蒂＝＊，敲声＝浊响
  >
  > 17 色泽＝青绿，根蒂＝＊，敲声＝清脆
  >
  > 18 色泽＝青绿，根蒂＝＊，敲声＝沉闷
  >
  > 19 色泽＝乌黑，根蒂＝＊，敲声＝浊响
  >
  > 20 色泽＝乌黑，根蒂＝＊，敲声＝清脆
  >
  > 21 色泽＝乌黑，根蒂＝＊，敲声＝沉闷
  >
  > 22 色泽＝＊，根蒂＝蜷缩，敲声＝浊响
  >
  > 23 色泽＝＊，根蒂＝蜷缩，敲声＝清脆
  >
  > 24 色泽＝＊，根蒂＝蜷缩，敲声＝沉闷
  >
  > 25 色泽＝＊，根蒂＝硬挺，敲声＝浊响
  >
  > 26 色泽＝＊，根蒂＝硬挺，敲声＝清脆
  >
  > 27 色泽＝＊，根蒂＝硬挺，敲声＝沉闷
  >
  > 28 色泽＝＊，根蒂＝稍蜷，敲声＝浊响
  >
  > 29 色泽＝＊，根蒂＝稍蜷，敲声＝清脆
  >
  > 30 色泽＝＊，根蒂＝稍蜷，敲声＝沉闷
  >
  > 31 色泽＝青绿，根蒂＝蜷缩，敲声＝浊响
  >
  > 32 色泽＝青绿，根蒂＝蜷缩，敲声＝清脆
  >
  > 33 色泽＝青绿，根蒂＝蜷缩，敲声＝沉闷
  >
  > 34 色泽＝青绿，根蒂＝硬挺，敲声＝浊响
  >
  > 35 色泽＝青绿，根蒂＝硬挺，敲声＝清脆
  >
  > 36 色泽＝青绿，根蒂＝硬挺，敲声＝沉闷
  >
  > 37 色泽＝青绿，根蒂＝稍蜷，敲声＝浊响
  >
  > 38 色泽＝青绿，根蒂＝稍蜷，敲声＝清脆
  >
  > 39 色泽＝青绿，根蒂＝稍蜷，敲声＝沉闷
  >
  > 40 色泽＝乌黑，根蒂＝蜷缩，敲声＝浊响
  >
  > 41 色泽＝乌黑，根蒂＝蜷缩，敲声＝清脆
  >
  > 42 色泽＝乌黑，根蒂＝蜷缩，敲声＝沉闷
  >
  > 43 色泽＝乌黑，根蒂＝硬挺，敲声＝浊响
  >
  > 44 色泽＝乌黑，根蒂＝硬挺，敲声＝清脆
  >
  > 45 色泽＝乌黑，根蒂＝硬挺，敲声＝沉闷
  >
  > 46 色泽＝乌黑，根蒂＝稍蜷，敲声＝浊响
  >
  > 47 色泽＝乌黑，根蒂＝稍蜷，敲声＝清脆
  >
  > 48 色泽＝乌黑，根蒂＝稍蜷，敲声＝沉闷
  >
  > 49 Ø

  **删除与正例不一致的假设、和（或）与反例一致的假设。进行学习：**

  > （1，（色泽＝青绿、根蒂＝蜷缩、敲声＝浊响），好瓜）
  >
  > 可以删除假设空间中的3、5、6、8、9、11-15、17-21、23-30、32-49
  >
  > （2，（色泽＝乌黑、根蒂＝蜷缩、敲声＝浊响），好瓜）
  >
  > 可以删除剩余假设空间中的2、10、16、31
  >
  > （3，（色泽＝青绿、根蒂＝硬挺、敲声＝清脆），坏瓜）
  >
  > 可以删除剩余假设空间中的1
  >
  > （4，（色泽＝乌黑、根蒂＝稍蜷、敲声＝沉闷），坏瓜）

  **学习过后剩余的假设为**

  > 4 色泽＝＊，根蒂＝蜷缩，敲声＝＊
  >
  > 7 色泽＝＊，根蒂＝＊，敲声＝浊响
  >
  > 22 色泽＝＊，根蒂＝蜷缩，敲声＝浊响

  **最终版本空间为：**

  ![版本空间](https://s1.ax2x.com/2018/04/01/ticAK.png)

* **归纳偏好**

  > 机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”。

  > "奥卡姆剃刀"：若有多个假设与观察一直，则选最简单的那个。

  * **简短的讨论**

    > **首先，我们假设一个算法为a，  而随机胡猜的算法为b。**假设样本空间**X**和假设空间**H**都是离散的。令**P**（h|**X，**￡a）代表算法￡a基于训练数据**X**产生假设h的概率，令**f**代表我们真实的目标函数。那么￡a在训练集之外的所有的样本上的误差为：

    ![](https://s1.ax2x.com/2018/04/01/t7w3h.png)

    > 两个求和号：类似于两个for()循环嵌套。

    > 算法**f**基于训练数据**X**产生假设￡a的概率，即误差。
    >
    > P(**x**)即**x**出现的概率。
    >
    > 若h(**x**)≠f(**x**)，则Ⅱ(.)取1，否则取0。即若h(**x**)≠f(**x**)，则计算一次求和，否则等于零，不用求和。

    ![](https://s1.ax2x.com/2018/04/01/tRhgG.jpg)

    > 大意为：误差=p(**x1**)的概率×h(**x**)与f(**x**)不匹配的概率×算法￡a基于训练数据**X**产生假设h的概率

    ​

    * 对所有可能的f按均匀分布对误差求和

    ![](https://s1.ax2x.com/2018/04/01/tRafQ.jpg)

    > 得到：对于任何两个算法a和b都有：  **∑fEote(a|X,f)=∑fEote(b|X,f)**
    >
    > 因此，不论算法a多聪明，算法b多笨拙，他们的期望竟然相同！！这就是**NFL**（没有免费的午餐  No Free Lunch Theorem）定理。
    >
    > 事实上，NFL定理架设了**f**的均匀分布，而实际情况并不是这样。NFL最重要的寓意，是让我们认识到，脱离具体问题，空谈“什么算法更好”毫无意义。
